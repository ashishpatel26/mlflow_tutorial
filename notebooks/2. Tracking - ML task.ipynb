{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# author: Vladimir Osin "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn            as nn\n",
        "import torch.optim         as optim\n",
        "import torch.nn.functional as F\n",
        "import mlflow              as mf\n",
        "import mlflow.pytorch\n",
        "import shutil\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision      import datasets, transforms\n",
        "from torch.autograd   import Variable"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task: Classification of Kuzushiji-MNIST numbers\n",
        "\n",
        "### A bit of History \n",
        "Recorded historical documents give us a peek into the past. We are able to glimpse the world before our time; and see its culture, norms, and values to reflect on our own. Japan has very unique historical pathway. Historically, Japan and its culture was relatively isolated from the West, until the Meiji restoration in 1868 where Japanese leaders reformed its education system to modernize its culture. \n",
        "\n",
        "This caused drastic changes in the Japanese language, writing and printing systems. Due to the modernization of Japanese language in this era, cursive Kuzushiji (くずし字) script is no longer taught in the official school curriculum. Even though Kuzushiji had been used for over 1000 years, most Japanese natives today cannot read books written or published over 150 years ago.\n",
        "\n",
        "The result is that there are hundreds of thousands of Kuzushiji texts that have been digitised but have never been transcribed, and can only currently be read by a few experts. We've built Kuzushiji-MNIST and sister datasets by taking handwritten characters from these texts and preprocessing them in a format similar to the MNIST dataset, to create easy to use benchmark datasets that are more modern and difficult to classify than the original MNIST dataset.\n",
        "\n",
        "[Kuzushiji-MNIST dataset on Kaggle](https://www.kaggle.com/anokas/kuzushiji)\n",
        "\n",
        "[Paper: Deep Learning for Classical Japanese Literature](https://arxiv.org/pdf/1812.01718.pdf)\n",
        "\n",
        "### Task description \n",
        "\n",
        "- Build hand-written characters recognition system using convolutional neural networks\n",
        "- Use MLflow for tracking experimetns \n",
        "\n",
        "![](https://raw.githubusercontent.com/rois-codh/kmnist/master/images/kmnist_examples.png)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parameters"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# our parameters\n",
        "args = {'batch_size'      : 10,      # input batch size for training\n",
        "        'test_batch_size' : 100,     # input batch size for testing\n",
        "        'epochs'          : 1,      # number of epochs to train\n",
        "        'seed'            : 42,      # fixing seed for reproducibility\n",
        "        'learning_rate'   : 0.03,    # learning rate for optimizer\n",
        "        'momentum'        : 0.1}     # momentum for optimizer\n",
        "\n",
        "# GPU or CPU training based on your device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('your device is {0}'.format(device))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Neural network architecture (CNN)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "class Awesome_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Awesome_Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10,  kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(-1, 320)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=0)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset and Dataloader"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# preparing dataset\n",
        "transform     = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "train_dataset = datasets.KMNIST('../data', train=True,  download=True, transform=transform)\n",
        "test_dataset  = datasets.KMNIST('../data', train=False, download=True, transform=transform)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# preparing loader\n",
        "train_loader = DataLoader(train_dataset, batch_size=args['batch_size'], shuffle=True, num_workers=1)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=args['test_batch_size'], shuffle=True, num_workers=1)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Model and Optimizer"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "model     = Awesome_Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=args['learning_rate'], momentum=args['momentum'])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and Testing functions"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "    model.train()\n",
        "    \n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if batch_idx % 1000  == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.data.item()))\n",
        "            step = epoch * len(train_loader) + batch_idx\n",
        "            mf.log_metric('train_loss',  loss.data.item())\n",
        "\n",
        "            \n",
        "def test(epoch):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct   = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output       = model(data)\n",
        "            test_loss   += F.nll_loss(output, target, reduction='sum').data.item() # sum up batch loss\n",
        "            pred         = output.data.max(1)[1] # get the index of the max log-probability\n",
        "            correct     += pred.eq(target.data).cpu().sum().item()\n",
        "\n",
        "    test_loss    /= len(test_loader.dataset)\n",
        "    test_accuracy = 100.0 * correct / len(test_loader.dataset)\n",
        "    \n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset), test_accuracy))\n",
        "        \n",
        "    mf.log_metric('test_loss',     test_loss)\n",
        "    mf.log_metric('test_accuracy', test_accuracy)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MLflow Experiment"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# let's connect to the tracking server \n",
        "mf.set_tracking_uri('http://127.0.0.1:5000')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# let's connect to the tracking server \n",
        "experiment_id = mf.set_experiment(\"Tracking - ML Task\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(args[\"seed\"])\n",
        "\n",
        "with mf.start_run():\n",
        "    \n",
        "    # creating a temporary folder \n",
        "    output_dir   = 'tmp/'\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    # logging parameters\n",
        "    mf.log_params(args)\n",
        "\n",
        "    # perform training and testing\n",
        "    for epoch in range(1, args['epochs'] + 1):\n",
        "        train(epoch)\n",
        "        test(epoch)\n",
        "    \n",
        "    # store trained model\n",
        "    mf.pytorch.log_model(model, \"model\")\n",
        "        \n",
        "    # logging all model artifacts \n",
        "    mf.log_artifacts(output_dir)\n",
        "    \n",
        "    # remove temporary folder\n",
        "    shutil.rmtree(output_dir)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "nteract": {
      "version": "0.15.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}